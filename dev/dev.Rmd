https://legis.senado.leg.br/dadosabertos/api-docs/swagger-ui/index.html#/Comiss%C3%A3o/detalhesColegiado

## intro

```{r ll}
sto::ll("sto dplyr rvest")
options(browser = "firefox")
detach("package:notasTaq", unload = TRUE)
ls("package:notasTaq")

baseURL <- "https://legis.senado.leg.br/dadosabertos/"
```
```{r build things}
devtools::build_readme() # locates your README.Rmd and builds it into a README.md
devtools::build_manual() # Create package pdf manual

devtools::clean_vignettes()
devtools::build_vignettes(pkg = ".")
devtools::build_vignettes("vignettes")
devtools::build_vignettes("vignettes/codifing.Rmd")
usethis::use_github()
usethis::use_github_links() #  If a package is already connected to a remote GitHub repository, usethis::use_github_links() can be called to just add the relevant links to DESCRIPTION.'use_github_links()'
# usethis::use_data_table() # creates all the boilerplate. https://usethis.r-lib.org/reference/use_data_table.html
# usethis::use_pkgdown() # is a function you run once and it does the initial, minimal setup necessary to start using pkgdown:

# devtools::build_articles() # Create package pdf manual
pkgdown::build_site_github_pages()

```
```{r URL}
URL <- "https://www25.senado.leg.br/web/atividade/notas-taquigraficas/-/notas/r/13611"
parser(URL)
```
```{r read html}
NT_html <- list.files("inst/extdata/", full.names=T) |> 
  read_html(encoding = "utf8")
```
```{r html_elem}
title <- NT_html |> html_elements("h1") |> html_text() |> grep("\\d+/\\d+", x=_, value = TRUE)
```
```{r }
NT_html |> 
  html_elements("#content") |>
  # html_elements(".justificado") |> 
  html_elements("b") |> 
  html_text()

  # texto <- NT_html |> rvest::html_nodes('.escriba-jq') |> rvest::html_text()
  texto <- NT_html |> rvest::html_elements(".justificado") |> rvest::html_text()
  alerta <- NT_html |> rvest::html_element('.alert') |> rvest::html_text()

  texto |> grep2("^([A] SR)", ic=F)
  texto[1] |> substr(1, 500)
 
 # breaking the text into vector elements at every "O SR|A SRA"
 ## method 1
  texto_vetores0 <- texto |> 
    # paste(collapse = " ") |> 
    gsub('[0-9]{2}\\:[0-9]{2}  R|\\(Pausa\\.\\)', '', )  |>
      gsub('(O SR|A SRA)\\.', 'ZZZVECTOR_\\1\\.', .)  |>
    strsplit(. , "ZZZVECTOR_")  |> 
    unlist()
## method 2
# Function to concatenate elements that do not start with "O SR|A SRA"
  collapse_non_sr <- function(vec) {
    vec %>%
    purrr::keep(~ !stringr::str_detect(., "^O SR|A SRA")) %>%
      paste(collapse = " ")
  }

# Apply the function
  collapse_non_sr(texto_vetores0) |> length()

 ## method 3
DF <-   tibble::tibble(txt = texto) |> 
    mutate(
      inic_frase = grepl(x = texto, "^(O SR|A SRA| A SRa)\\b") 
    )
  DF |> 
  group_by(group =  cumsum(inic_frase)) |> 
  summarise(txt  = paste(txt, collapse = " ")) |> 
      mutate(
      in_middle = grepl(x=texto, ".+(O SR|A SRA|A SRa)\\b" )
  )
  # TODO
    # mutate(
      # txt = ifelse(!inic_frase, paste( collapse) )
    # txt = ifelse(in_middle, tidyr::pivot_longer(txt, cols =  ) )
    # txt = ifelse(in_middle,  ) ))


# TODO new lines only when begin with <b>

  # limpando a linha 1, se ela contiver o indesejável texto abaixo (sempre vem)
  reuniao <- texto_vetores0[1] %>% gsub(".* ([0-9]+)ª.*", "\\1",.)
  regex.1linha <- "\n\n+.*@import.*Texto com revisão +"

  if (grepl(regex.1linha, texto_vetores0[1])){
    texto_vetores = texto_vetores0[-1]
  } else {
    texto_vetores = texto_vetores0
  }


  ExpReg <- '(O SR|A SRA)\\. ([A-ZÀ-Ÿ \\.]+)(\\(.*?\\)| ?)([-   ––]{3})(.*)'
  vetor_nomes <- unlist(str_extract_all(texto_vetores, ExpReg))
  nome <- gsub(ExpReg,'\\2', texto_vetores) %>% gsub(' $','',.)
  # unique(nome)
  funcao_bloco = gsub(ExpReg,'\\3', texto_vetores)
  fala <- gsub(ExpReg,'\\5', texto_vetores)
```
```{r fun from_html_2_df}
fun from_html_2_df <- function(html_page) {

  title <- NT_html |> html_elements("h1") |> 
    html_text() |> grep("\\d+/\\d+", x=_, value = TRUE)

  texto <- NT_html |> rvest::html_elements(".justificado") |> rvest::html_text()
  # alerta <- NT_html %>% rvest::html_element('.alert') %>% rvest::html_text()
 
 # breaking the text into vector elements at every "O SR|A SRA"
 ## method 1
  texto_vetores0 <- texto |> 
    # paste(collapse = " ") |> 
    gsub('[0-9]{2}\\:[0-9]{2}  R|\\(Pausa\\.\\)', '', )  |>
      gsub('(O SR|A SRA)\\.', 'ZZZVECTOR_\\1\\.', .)  |>
    strsplit(. , "ZZZVECTOR_")  |> 
    unlist()
}


```
```{r test break}
NT_html <- list.files("inst/extdata/", full.names=T) |> readLines()
tempFile <- tempfile()

NT_html |> 
  gsub(x=_, "(<b>)", "<break_line>\\1") |> 
  writeLines( tempFile)

tempFile |> #read_html(encoding = "utf8")
  read_html(encoding = "utf8") |>
  html_elements("break_line") |>
  html_text()
  # html_attrs(".break_line")

NT_html |> 
  gsub(x=_, "(<b>)", "<break_line>\\1") |>
  paste(collapse = " ") |>
  xml2::read_html(encoding = "utf8") |>
  html_elements("break_line") |>
  html_text()


NT_html
```
```{r fun from_html_2_df}
from_html_2_df <- function(html_page) {
# html_page = NT_html 

  text <- html_page |> 
  # NT_html |> 
    as.character() |>
    gsub(x=_, "(<b>)", "<break_line>\\1") |>
    paste(collapse = " ") |>
    xml2::read_html(encoding = "utf8") |>
    html_elements("break_line") |>
    html_text()

  # transform data type of input
  if (typeof(html_page) == "character") {
    html_page <- xml2::read_html(
      paste(html_page, collapse = " "), 
      encoding = "utf8")
  }

  title <- html_page |> 
    # NT_html |> 
      html_elements("h1") |> 
      html_text() |> 
      grep("\\d+/\\d+", x=_, value = TRUE)

  tibble(title, text)
 
}

NT_html <- list.files("inst/extdata/", full.names=T) |> readLines()
from_html_2_df(NT_html)
NT_html <- list.files("inst/extdata/", full.names=T) |> read_html(encoding = "utf8")
from_html_2_df(NT_html)
NT_html |> typeof()
NT_html |> class()
```
```{r parser}
scraped_page |> parser()
```
```{r }
URL <- "https://www25.senado.leg.br/web/atividade/notas-taquigraficas/-/notas/r/13611"
pagina <- rvest::read_html(URL)
class(pagina)
typeof(pagina)
html_page <- pagina
rm(html_page)
parse(html_page)
parse(URL)
```
```{r new parse}

parse_NT <- function(html_page, save_as) {
  # html_page <- NT_html
  # html_page <- URL

  if (any(class(html_page) %in% "xml_document" )){
      DF <- extract_metadata(html_page)
  } else{ 
      DF <- html_page |> 
        rvest::read_html() |>
        extract_metadata()
  }
  # save_as
  if ( exists("save_as") ){
      DF
  } else {
      save_csv_rds(DF, save_as)
  }

  return(DF)
}

rm(parse_NT)
file_html <- list.files("inst/extdata/", full.names=TRUE)
NT_html <- xml2::read_html(file_html,encoding = "utf8")
parse_NT(NT_html)

URL <- "https://www25.senado.leg.br/web/atividade/notas-taquigraficas/-/notas/r/13611"
parse_NT(URL)

```
```{r fun extract metadata}
dir_exs <- "./inst/extdata/"
file_name <- paste0(dir_exs, "NT_13219.html")

# download.file(URL, file_name)
DF <- rvest::read_html(file_name, encoding = "utf8") 
DF <- from_html_2_df(DF)

rgx_date <- "\\d+\\/\\d+\\/\\d+"
rgx_date <- "\\d+/\\d+/\\d+"
rgx_date <- "(\\d+/){2}\\d+"
DF$Title |> stringr::str_extract( rgx_date) #|> as.vector(),
# DF2 <- 
DF |> #dplyr::select(text) |> 
    dplyr::mutate(
      Date = stringr::str_extract(Title, rgx_date) |> as.Date("%d/%m/%Y"), 
      Title = stringr::str_remove(Title, rgx_date),
      # reunion_n = stringr::str_replace(Titre, ".* - (\\d+) - .*", "\\1"),
      reunion_n = Title |> 
        strsplit("-") |> unlist() |> 
        grep("\\d+", x=_, value = TRUE) |> 
        stringr::str_trim(), 
    # stringr::str_replace(, ".* - (\\d+) - .*", "\\1"),
      Title =  Title |> 
        strsplit("-") |> 
        unlist() |>
        dplyr::nth(3) |> 
        stringr::str_trim(),
      metadata = text |> 
        substring(1,300) |> 
        # stringr::str_extract_all( rgx_metaData, simplify = TRUE) |>
        stringr::str_extract_all( ".*\\)", simplify = TRUE) |> 
        as.vector(),
        # gsub(x=_, "(.*\\)).*", "\\1"),
      # if metadata is empty, because text pattern does not has ), then new rule to extract
      metadata = ifelse(metadata == "", 
        text |> 
          substring(1,300) |> 
          stringr::str_extract_all( ".* \\W ", simplify = TRUE) ,
        metadata),
      gentilic = metadata |>
        substring(1,10) |> 
        stringr::str_extract_all( "([sSdD][Rr][aA]?)", simplify = TRUE) |> 
        as.vector() ,
        # gsub(x=_, ".*([sSdD][Rr][aA]?).*", "\\1") ,
      # checar se contem SR SRA, if not, return empty
        # gentilic = ifelse(
        #   grepl("[sSdD][Rr][aA]?", gentilic), 
        # gentilic,""),
      speaker = metadata |>
        gsub(x=_,"^[OAoa] [SsRr]{2}[aA]?[\\. ]+","") |> # erase "O SR" at the beginning
        gsub(x=_," \\W $","") |> # erase " - " at the end
        # gsub(x=_," - $","") |> # erase "O SR" at the beginning
        gsub(x=_, " +\\(.*", "")  |> 
        trimws(),
      metadata2 = metadata |> # metadata inside parenthesis, e.g,: "("Dr. Hiran. Bloco Parlamentar Aliança/PP - RR)"
        stringr::str_extract_all("\\(.*\\)", simplify = TRUE) |>
         # stringr::str_remove_all( "\\\\(")
        # gsub(x=_, r"([\(\)])", ""),
        stringr::str_remove_all( 
          # r"([\(\)])",
          "\\(|\\)"),
      party_UF = metadata2 |>
        stringr::str_extract_all("\\W\\w+ \\W [A-Z]{2}", simplify = TRUE) |>
        stringr::str_remove_all("\\/"), 
        # gsub(x=_,"\\/", ""),
      party = party_UF |> 
        gsub(x=_, "(\\w*)[\\W -]+.*[A-Z]{2}", "\\1" ),
      state =  party_UF |> 
        gsub(x=_, "(\\w*)[\\W -]+.*([A-Z]{2})", "\\2"),
      # complement = TODO 
      block = metadata2 |> 
        # gsub(x=_, "^([A-Za-z ]+)[\\/\\-].*", "\\1")
        gsub(x=_, "^(.*)\\/.*", "\\1")
  ) |> dplyr::select(-gentilic, -text) #
## comissões
DF2|> dplyr::count(party_UF)
DF2
```
```{r }
codigo <- "449"
baseURL <- "https://legis.senado.leg.br/dadosabertos/"
URL  <- paste0(baseURL, "comissao/", codigo)

http <- httr::GET(URL)

http$request
http$headers
content <- http$content |> rawToChar() |> jsonlite::fromJSON()

content$ComissoesCongressoNacional |> 
  listviewer::jsonedit()

content$ComissoesCongressoNacional$Metadados
content$ComissoesCongressoNacional$noNamespaceSchemaLocation

content$ComissoesCongressoNacional$Colegiados |> 
  # str()
  dplyr::bind_rows()
```

/dadosabertos/comissao/agenda/{dataInicio}/{dataFim}
```{r agenda}
"/dadosabertos/comissao/agenda/{dataInicio}/{dataFim}"
dataInicio = 20241218
dataFim = 20251218

dataInicio = 20241210
dataFim = 20241220

v =2
URL <- paste0(baseURL, "comissao/agenda/", dataInicio,"/", dataFim, "?v=", v)

http <- httr::GET(URL)
http <- httr::GET(URL, accept = 'text/csv')
text <- http$content |> rawToChar() 

txt_list <- text |> jsonlite::fromJSON()

txt_list$AgendaReuniao$noNamespaceSchemaLocation
txt_list$AgendaReuniao$Metadados
txt_list$AgendaReuniao$reunioes |> 
  substring(1,10000) 
  # readLines(n=5) |>  
  # readLines() |>  
  # head(1)
  # read.csv(text=_)

http <- httr::GET(URL, accept = 'text/json')
text <- http$content |> rawToChar() |> jsonlite::fromJSON()

txt_list$AgendaReuniao$noNamespaceSchemaLocation
txt_list$AgendaReuniao$Metadados
txt_list$AgendaReuniao$reunioes |> parse() |> eval()


```



/dadosabertos/comissao/agenda/{dataReferencia}
```{r agenda/{dataReferencia}
dataReferencia = 20241218 
v = 2
paste0(baseURL, "comissao/agenda/", dataReferencia, "?v=", v)

http <- httr::GET(URL, accept = 'application/json')

http <- httr::GET(URL, accept = 'application/csv')
```

/dadosabertos/comissao/agenda/mes/{mesReferencia}
```{r mesReferencia}
mesReferencia <- 202412
v=2 
# glue::glue("/dadosabertos/comissao/agenda/{dataInicio}/{dataFim}")
URL <- glue::glue("{baseURL}comissao/agenda/mes/{mesReferencia}")
page_ <- httr::GET(URL)
page_

http <- httr::GET(URL, accept = 'application/json')
DF <- http$content |> rawToChar() |> jsonlite::fromJSON() |> dplyr::bind_rows() 
DF$reunioes$reuniao |> names() 
DF$reunioes$reuniao |> dplyr::bind_rows() 

http <- httr::GET(URL, accept = 'application/csv')
# DF <- 
  http$content |> rawToChar() |> 
   jsonlite::fromJSON()
  read.csv2(text = _) 
  readr::read_csv2() 
|> dplyr::bind_rows() 
```

/dadosabertos/comissao/reuniao/{codigoReuniao}
```{r reuniao/codigoReuniao}
codigo <- 13219
URL <- paste0(baseURL, "comissao/reuniao/", codigo)
http <- httr::GET(URL)
content <- http$content |> rawToChar() |> jsonlite::fromJSON()
content |> dplyr::bind_rows()
```

/dadosabertos/comissao/{codigo}
```{r comissao/codigo}
codigo <- 449
URL <- paste0(baseURL, "comissao/", codigo, '?v=3' )
# http <- httr::GET(URL)
http <- httr::GET(URL, accept = 'application/json')
# content <- http$content |> rawToChar() |> rvest::read_html()
DF <- http$content |> rawToChar() |> jsonlite::fromJSON()# 
DF |> dplyr::bind_rows() 
DF$Metadados
DF$Colegiados |> dplyr::bind_rows() 
DF$Colegiados$Colegiado |> dplyr::bind_rows() 
```

### notas
/dadosabertos/comissao/reuniao/notas/{codigoReuniao}
```{r notas/codigoReuniao}
codigo <- 13219
URL <- paste0(baseURL, "comissao/reuniao/notas/", codigo)
http <- httr::GET(URL)
content <- http$content |> rawToChar() |> jsonlite::fromJSON()

page <- content$NotasTaquigraficasReuniao$UrlNotasTaquigraficas |> rvest::read_html()
page |> extract_metadata()


```

## Partidos
"Obtém a lista dos Partidos Políticos em atividade e/ou extintos no Senado Federal"
"/dadosabertos/composicao/lista/partidos"
```{r partidos}
"/dadosabertos/composicao/lista/partidos"
URL <- paste0(baseURL, "composicao/lista/partidos")
http <- httr::GET(URL)

partidos <- http$content |> rawToChar() |> jsonlite::fromJSON()  #|> dplyr::bind_rows()

partidos |> listviewer::jsonedit()

partidos$ListaPartidos$Partidos[[1]]|> tibble::as_tibble()

partidos$ListaPartidos$Partidos[[1]]|> tibble::as_tibble() |> 
  dplyr::filter(DataExtincao == "NA")

## transforming code above into function
# get_parties <- function() {  
#     URL <- paste0(baseURL, "composicao/lista/partidos")
#     http <- httr::GET(URL)
#
#     partidos <- http$content |> rawToChar() |> jsonlite::fromJSON()  #|> dplyr::bind_rows()
#
#     partidos |> listviewer::jsonedit()
#
#     partidos$ListaPartidos$Partidos  |> tibble::as_tibble()
# }
#
# get_parties()
```


## CPI 
/dadosabertos/comissao/cpi/{comissao}/requerimentos

https://legis.senado.leg.br/dadosabertos/comissao/cpi/cpibets/requerimentos?pagina=1&tamanho=20
```{r }
"/dadosabertos/comissao/cpi/{comissao}/requerimentos"
comissao <- "cpibets"
pagina = 1 
tamanho = 

URL <- paste0(baseURL, "comissao/cpi/", comissao, "/requerimentos?pagina=", pagina)
http <- httr::GET(URL)
content <- http$content |> rawToChar() |> rvest::read_html()
content_elements <- content |>  html_elements("*") |> html_name()
# content_txt <- content |> rvest::html_text(trim = T)
list_resp <-  lapply(content_elements, \(i)  { content |> rvest::html_nodes(i)|> rvest::html_text() })
names(list_resp) <- content_elements
list_resp
# purrr::map(content_elements, \(i)  { content |> rvest::html_nodes(i)|> rvest::html_text() })
# for (i in content_elements) { content |> rvest::html_nodes(i) |> rvest::html_text() }

content_txt <- content |> rvest::html_nodes("detail")|> rvest::html_text()
#content_txt <- 
  content |> rvest::html_nodes("instance")|> rvest::html_text()
content_txt_bool <- grepl(x=content_txt, pattern = "(?i)não encontrado")
if (content_txt_bool) { message("Not Found: " ,content_txt)}
content <- http$content |> rawToChar() |> jsonlite::fromJSON()


```

## outros / confusao / shuffle / adhoc

```{r }
dir_exs <- "./inst/extdata/"
file_name <- paste0(dir_exs, "NT_13219.html")

# download.file(URL, file_name)
pagina <- rvest::read_html(file_name, encoding = "utf8") |> 
  parse_TN()

pagina
```
```{r }
DF |> # dplyr::select(text) |> 
    dplyr::mutate(
      metadata = text |> 
        substring(1,300) |>
      gsub(x=_, "(.*\\)).*", "\\1")
)
```
```{r }

s_extract <- function(txt, pattern) {
  txt |> 
    gsub(x=_,
        # paste0("(", pattern ,").*"),
        paste0("(.*)(", pattern, ")(.*)"),
      "\\2")
}

"O SR. PRESIDENTE (Jorge Kajuru. Bloco Parlamentar da Resistência Democrática/PSB - GO) - Eu quero apenas, aproveitando a pergunta do Senador Eduardo Girão, nosso Vice-Presidente, como a ideia foi minha, em uma entrevista na Rede TV com o Presidente Lula, em que ele concordou plenamente, o que ele colocou para o Brasil inteiro é que ele vai propor, através de decreto-lei, que essas apostas tenham as seguintes proibições: em cartão amarelo, em cartão vermelho, em arremesso manual, em escanteio, em gols, em pênaltis. Que somente possam as empresas legalizadas - que já estão, aliás, algumas delas -, que apenas possa se apostar no resultado do jogo. Perfeito? Então, apenas fazendo aqui esta observação. " |> s_extract(".*\\)")
```
